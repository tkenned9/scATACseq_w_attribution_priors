{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-1f757cfd0ad8>:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm.tqdm_notebook()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6bd3ad75134e068768d6be8591838f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "|<bar/>| 0/? [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "import scipy.stats\n",
    "import scipy.ndimage\n",
    "import sklearn.metrics\n",
    "import pyfaidx\n",
    "import pyBigWig\n",
    "import tqdm\n",
    "import project as proj\n",
    "import importlib\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "importlib.reload(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_path = './data/vcm_reads.bw'\n",
    "open_regions_path = './data/vcm_peaks.bed'\n",
    "chrom_sizes_path = './data/hg38.canon.chrom.sizes'\n",
    "ref_fasta_path = './data/hg38.fasta'\n",
    "reads = pyBigWig.open(reads_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving array of shape (70577, 3) in file: ./data/train_regions.npy\n",
      "saving array of shape (6424, 3) in file: ./data/val_regions.npy\n",
      "saving array of shape (8385, 3) in file: ./data/test_regions.npy\n"
     ]
    }
   ],
   "source": [
    "def create_label_file(filepath, set_chroms, regions_df):\n",
    "    set_regions = regions_df[regions_df['chrom'].isin(set_chroms)].copy().values\n",
    "    print(\"saving array of shape \" + str(set_regions.shape) + \" in file: \" + filepath)\n",
    "    np.save(filepath, set_regions)\n",
    "    \n",
    "chrom_sizes = {}\n",
    "with open(chrom_sizes_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        chrom, size = line.strip().split()\n",
    "        if len(chrom) > 5 or chrom in (\"chrY\", \"chrM\"):\n",
    "            continue\n",
    "        chrom_sizes[chrom] = int(size)\n",
    "    \n",
    "test_chroms = ['chr1']\n",
    "val_chroms = ['chr2']\n",
    "train_chroms = [chrom for chrom in chrom_sizes.keys() if chrom not in (test_chroms + val_chroms)]    \n",
    "open_regions = pd.read_csv(open_regions_path, sep=\"\\t\", header=None, names=[\"chrom\", \"start\", \"end\"])\n",
    "train_regions_npy_path = \"./data/train_regions.npy\"\n",
    "val_regions_npy_path = \"./data/val_regions.npy\"\n",
    "test_regions_npy_path = \"./data/test_regions.npy\"  \n",
    "create_label_file(train_regions_npy_path, train_chroms, open_regions)\n",
    "create_label_file(val_regions_npy_path, val_chroms, open_regions)\n",
    "create_label_file(test_regions_npy_path, test_chroms, open_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions adapted from https://github.com/amtseng/fourier_attribution_priors/blob/master/src/model/train_profile_model.py\n",
    "\n",
    "def run_epoch(data_loader, mode, model, epoch_num, batch_size, att_prior_loss_weight=0,\n",
    "              num_tasks=1, counts_loss_weight = 25, freq_limit = 200,\n",
    "              limit_softness = 0.2, att_prior_grad_smooth_sigma = 3,\n",
    "              input_length=1346, input_depth=4, profile_length=1000,optimizer=None, \n",
    "              return_data=False):\n",
    "    \n",
    "    \n",
    "    assert mode in (\"train\", \"eval\")\n",
    "    if mode == \"train\":\n",
    "        assert optimizer is not None\n",
    "    else:\n",
    "        assert optimizer is None \n",
    "    \n",
    "    data_loader.shuffle_data()\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    t_iter = tqdm.tqdm(\n",
    "        data_loader, total=num_batches, desc=\"\\tLoss: ---\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if mode == \"train\":\n",
    "        model.train()  # Switch to training mode\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    batch_losses, corr_losses, att_losses = [], [], []\n",
    "    prof_losses, count_losses = [], []\n",
    "    \n",
    "    input_seqs_array = []\n",
    "    profiles_array = []\n",
    "    for input_seqs, profiles in t_iter:\n",
    "        input_seqs = proj.place_tensor(torch.tensor(input_seqs)).float()\n",
    "\n",
    "        profiles = proj.place_tensor(torch.tensor(profiles)).float()\n",
    "        profiles = profiles.view(profiles.shape[0],1,-1,1)\n",
    "        \n",
    "        input_seqs_array.append(input_seqs)\n",
    "        profiles_array.append(profiles)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "        elif att_prior_loss_weight > 0:\n",
    "            # Not training mode, but we still need to zero out weights because\n",
    "            # we are computing the input gradients\n",
    "            model.zero_grad()\n",
    "        \n",
    "        if att_prior_loss_weight > 0:\n",
    "            input_seqs.requires_grad=True\n",
    "            logit_pred_profs, log_pred_counts = model(input_seqs)\n",
    "\n",
    "            norm_logit_pred_profs = logit_pred_profs - torch.mean(logit_pred_profs, dim=2, keepdim=True) \n",
    "            pred_prof_probs = profile_logits_to_log_probs(logit_pred_profs).detach()\n",
    "            weighted_norm_logits = norm_logit_pred_profs * pred_prof_probs\n",
    "\n",
    "            input_grads, = torch.autograd.grad(\n",
    "                weighted_norm_logits, input_seqs,\n",
    "                grad_outputs=proj.place_tensor(\n",
    "                    torch.ones(weighted_norm_logits.size())\n",
    "                ),\n",
    "                retain_graph=True, create_graph=True\n",
    "            )\n",
    "            \n",
    "            input_grads = input_grads*input_seqs\n",
    "            status = proj.place_tensor(torch.tensor(np.ones(input_grads.shape[0])))\n",
    "            input_seqs.requires_grad = False\n",
    "            \n",
    "            corr_loss, prof_loss, count_loss = model.correctness_loss(\n",
    "                profiles, logit_pred_profs, log_pred_counts, \n",
    "                counts_loss_weight, return_separate_losses=True\n",
    "            )\n",
    "            att_loss = model.fourier_att_prior_loss(\n",
    "                status, input_grads, freq_limit,\n",
    "                limit_softness, att_prior_grad_smooth_sigma\n",
    "            )\n",
    "            loss = corr_loss + att_prior_loss_weight*att_loss\n",
    "            \n",
    "        else:\n",
    "            logit_pred_profs, log_pred_counts = model(input_seqs)\n",
    "            status, input_grads = None, None\n",
    "            loss, prof_loss, count_loss = model.correctness_loss(\n",
    "                profiles, logit_pred_profs, log_pred_counts, \n",
    "                counts_loss_weight, return_separate_losses=True\n",
    "            )\n",
    "            corr_loss = loss\n",
    "            att_loss = torch.zeros(1)\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            loss.backward()  # Compute gradient\n",
    "            optimizer.step()  # Update weights through backprop\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        corr_losses.append(corr_loss.item())\n",
    "        att_losses.append(att_loss.item())\n",
    "        prof_losses.append(prof_loss.item())\n",
    "        count_losses.append(count_loss.item())\n",
    "        t_iter.set_description(\n",
    "            \"\\tLoss: %6.4f\" % loss.item()\n",
    "        )\n",
    "        \n",
    "    return batch_losses, corr_losses, att_losses, prof_losses, count_losses, input_seqs_array, profiles_array\n",
    "    \n",
    "\n",
    "def train_model(train_loader, val_loader, model, batch_size, lr, num_epochs, charts_path,\n",
    "               model_name, att_prior_loss_weight=0, early_stopping = False, early_stop_hist_len = 3, \n",
    "                early_stop_min_delta = .001, torch_seed = 3342):\n",
    "    \n",
    "    writer = SummaryWriter(charts_path)\n",
    "    \n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    torch.manual_seed(torch_seed)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    if early_stopping:\n",
    "        val_epoch_loss_hist = []\n",
    "    \n",
    "    best_val_epoch_loss, best_model_state = float(\"inf\"), None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if torch.cuda.is_available:\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        t_batch_losses, t_corr_losses, t_att_losses, t_prof_losses, t_count_losses, input_seqs_array, profiles_array = run_epoch(\n",
    "            train_loader, \"train\", model, epoch, batch_size, att_prior_loss_weight, optimizer=optimizer\n",
    "        )\n",
    "        \n",
    "        train_epoch_loss = np.nanmean(t_batch_losses)\n",
    "        print(\n",
    "            \"Train epoch %d: average loss = %6.10f\" % (\n",
    "                epoch + 1, train_epoch_loss\n",
    "            )\n",
    "        )\n",
    "        \n",
    "            \n",
    "        v_batch_losses, v_corr_losses, v_att_losses, v_prof_losses, v_count_losses, input_seqs_array, profiles_array = run_epoch(\n",
    "                val_loader, \"eval\", model, epoch, batch_size, att_prior_loss_weight\n",
    "        )\n",
    "        \n",
    "        val_epoch_loss = np.nanmean(v_batch_losses)\n",
    "        print(\n",
    "            \"Valid epoch %d: average loss = %6.10f\" % (\n",
    "                epoch + 1, val_epoch_loss\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    \n",
    "        writer.add_scalars(\"Loss\", {\"train\": train_epoch_loss, \"val\": val_epoch_loss}, epoch)\n",
    "        writer.add_scalars(\"Correctness_Loss\", {\"train\": np.nanmean(t_corr_losses), \"val\": np.nanmean(v_corr_losses)}, epoch)\n",
    "        writer.add_scalars(\"Attribution_Prior_Loss\", {\"train\": np.nanmean(t_att_losses), \"val\": np.nanmean(v_att_losses)}, epoch)\n",
    "        writer.add_scalars(\"Profile_Loss\", {\"train\": np.nanmean(t_prof_losses), \"val\": np.nanmean(v_prof_losses)}, epoch)\n",
    "        writer.add_scalars(\"Counts_Loss\", {\"train\": np.nanmean(t_count_losses), \"val\": np.nanmean(v_count_losses)}, epoch)\n",
    "\n",
    "\n",
    "        model_path = \"%s_epoch_%d.pt\" % (model_name, epoch + 1+19)\n",
    "        save_model(model, model_path)\n",
    "    \n",
    "        if np.isnan(train_epoch_loss) and np.isnan(val_epoch_loss):\n",
    "            print(\"Both NaN\")\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25\n",
    "charts_path = \"runs/exp10\"\n",
    "model_path = \"trained_models/exp10\"\n",
    "train_model(proj.DataLoader(train_regions_npy_path, batch_size=batch_size), proj.DataLoader(val_regions_npy_path,batch_size=batch_size),\n",
    "            proj.ProfilePredictor(), batch_size, learning_rate, num_epochs, charts_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25\n",
    "charts_path = \"runs/exp10_prior\"\n",
    "model_path = \"trained_models/exp10_prior\"\n",
    "att_loss_w = 280\n",
    "\n",
    "train_model(proj.DataLoader(train_regions_npy_path, batch_size=batch_size), proj.DataLoader(val_regions_npy_path,batch_size=batch_size),\n",
    "            proj.ProfilePredictor(), batch_size, learning_rate, num_epochs, charts_path, model_path, att_prior_loss_weight=att_loss_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
