{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "import scipy.stats\n",
    "import scipy.ndimage\n",
    "import sklearn.metrics\n",
    "import pyfaidx\n",
    "import pyBigWig\n",
    "import tqdm\n",
    "import project as proj\n",
    "import importlib\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_path = './data/vcm_reads.bw'\n",
    "open_regions_path = './data/vcm_peaks.bed'\n",
    "chrom_sizes_path = './data/hg38.canon.chrom.sizes'\n",
    "ref_fasta_path = './data/hg38.fasta'\n",
    "reads = pyBigWig.open(reads_path)\n",
    "train_regions_npy_path = \"./data/train_regions.npy\"\n",
    "val_regions_npy_path = \"./data/val_regions.npy\"\n",
    "test_regions_npy_path = \"./data/test_regions.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function as in Training.ipynb\n",
    "def run_epoch(data_loader, mode, model, epoch_num, batch_size, att_prior_loss_weight=0,\n",
    "              num_tasks=1, counts_loss_weight = 25, freq_limit = 200,\n",
    "              limit_softness = 0.2, att_prior_grad_smooth_sigma = 3,\n",
    "              input_length=1346, input_depth=4, profile_length=1000,optimizer=None, \n",
    "              return_data=False, cuda=0, shuffle=True):\n",
    "    \n",
    "    assert mode in (\"train\", \"eval\")\n",
    "    if mode == \"train\":\n",
    "        assert optimizer is not None\n",
    "    else:\n",
    "        assert optimizer is None \n",
    "    \n",
    "    if shuffle:\n",
    "        data_loader.shuffle_data()\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    t_iter = tqdm.tqdm(\n",
    "        data_loader, total=num_batches, desc=\"\\tLoss: ---\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if mode == \"train\":\n",
    "        model.train()  # Switch to training mode\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    batch_losses, corr_losses, att_losses = [], [], []\n",
    "    prof_losses, count_losses = [], []\n",
    "    \n",
    "    input_seqs_array = []\n",
    "    profiles_array = []\n",
    "    for input_seqs, profiles in t_iter:\n",
    "        input_seqs = proj.place_tensor(torch.tensor(input_seqs), cuda).float()\n",
    "\n",
    "        profiles = proj.place_tensor(torch.tensor(profiles), cuda).float()\n",
    "        profiles = profiles.view(profiles.shape[0],1,-1,1)\n",
    "        \n",
    "        input_seqs_array.append(input_seqs)\n",
    "        profiles_array.append(profiles)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "        elif att_prior_loss_weight > 0:\n",
    "            # Not training mode, but we still need to zero out weights because\n",
    "            # we are computing the input gradients\n",
    "            model.zero_grad()\n",
    "        \n",
    "        if att_prior_loss_weight > 0:\n",
    "            input_seqs.requires_grad=True\n",
    "            logit_pred_profs, log_pred_counts = model(input_seqs)\n",
    "\n",
    "            norm_logit_pred_profs = logit_pred_profs - torch.mean(logit_pred_profs, dim=2, keepdim=True) \n",
    "            pred_prof_probs = proj.profile_logits_to_log_probs(logit_pred_profs).detach()\n",
    "            weighted_norm_logits = norm_logit_pred_profs * pred_prof_probs\n",
    "\n",
    "            input_grads, = torch.autograd.grad(\n",
    "                weighted_norm_logits, input_seqs,\n",
    "                grad_outputs=proj.place_tensor(\n",
    "                    torch.ones(weighted_norm_logits.size()), cuda\n",
    "                ),\n",
    "                retain_graph=True, create_graph=True\n",
    "            )\n",
    "            \n",
    "            input_grads = input_grads*input_seqs\n",
    "            status = proj.place_tensor(torch.tensor(np.ones(input_grads.shape[0])), cuda)\n",
    "            input_seqs.requires_grad = False\n",
    "            \n",
    "            corr_loss, prof_loss, count_loss = model.correctness_loss(\n",
    "                profiles, logit_pred_profs, log_pred_counts, \n",
    "                counts_loss_weight, return_separate_losses=True\n",
    "            )\n",
    "            att_loss = model.fourier_att_prior_loss(\n",
    "                status, input_grads, freq_limit,\n",
    "                limit_softness, att_prior_grad_smooth_sigma\n",
    "            )\n",
    "            loss = corr_loss + att_prior_loss_weight*att_loss\n",
    "            \n",
    "        else:\n",
    "            logit_pred_profs, log_pred_counts = model(input_seqs)\n",
    "            status, input_grads = None, None\n",
    "            loss, prof_loss, count_loss = model.correctness_loss(\n",
    "                profiles, logit_pred_profs, log_pred_counts, \n",
    "                counts_loss_weight, return_separate_losses=True\n",
    "            )\n",
    "            corr_loss = loss\n",
    "            att_loss = torch.zeros(1)\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            loss.backward()  # Compute gradient\n",
    "            optimizer.step()  # Update weights through backprop\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        corr_losses.append(corr_loss.item())\n",
    "        att_losses.append(att_loss.item())\n",
    "        prof_losses.append(prof_loss.item())\n",
    "        count_losses.append(count_loss.item())\n",
    "        t_iter.set_description(\n",
    "            \"\\tLoss: %6.4f\" % loss.item()\n",
    "        )\n",
    "        \n",
    "    return batch_losses, corr_losses, att_losses, prof_losses, count_losses, input_seqs_array, profiles_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restore model and run epoch\n",
    "val_loader = proj.DataLoader(val_regions_npy_path, batch_size=1)\n",
    "val_model = proj.restore_model(\"./trained_models/exp5_epoch_17.pt\", 1)\n",
    "val_model.cuda(1)\n",
    "val_model.eval()\n",
    "\n",
    "val_batch_losses, val_corr_losses, val_att_losses, val_prof_losses, val_count_losses, val_input_seqs_array, val_profiles_array = run_epoch(val_loader, 'eval', val_model, 0, 1, cuda=1, shuffle=False)\n",
    "print(\"VALIDATION\")\n",
    "print(\"overall\")\n",
    "print(np.nanmean(val_batch_losses))\n",
    "print(\"correctness\")\n",
    "print(np.nanmean(val_corr_losses))\n",
    "print(\"attribution\")\n",
    "print(np.nanmean(val_att_losses))\n",
    "print(\"profile\")\n",
    "print(np.nanmean(val_prof_losses))\n",
    "print(\"count\")\n",
    "print(np.nanmean(val_count_losses))\n",
    "\n",
    "print(\"------------------\")\n",
    "\n",
    "test_loader = proj.DataLoader(test_regions_npy_path,batch_size=1)\n",
    "test_model = proj.restore_model(\"./trained_models/exp5_epoch_17.pt\", 1)\n",
    "test_model.cuda(1)\n",
    "test_model.eval()\n",
    "test_batch_losses, test_corr_losses, test_att_losses, test_prof_losses, test_count_losses, test_input_seqs_array, test_profiles_array = run_epoch(test_loader, 'eval', test_model, 0, 1, cuda=1, shuffle=False)\n",
    "print(\"TESTING\")\n",
    "print(\"overall\")\n",
    "print(np.nanmean(test_batch_losses))\n",
    "print(\"correctness\")\n",
    "print(np.nanmean(test_corr_losses))\n",
    "print(\"attribution\")\n",
    "print(np.nanmean(test_att_losses))\n",
    "print(\"profile\")\n",
    "print(np.nanmean(test_prof_losses))\n",
    "print(\"count\")\n",
    "print(np.nanmean(test_count_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restore model and run epoch\n",
    "val_loader = proj.DataLoader(val_regions_npy_path, batch_size=1)\n",
    "val_model = proj.restore_model(\"./trained_models/exp6_prior_epoch_20.pt\", 1)\n",
    "val_model.cuda(1)\n",
    "val_model.eval()\n",
    "\n",
    "print(\"WITH PRIOR\")\n",
    "\n",
    "val_batch_losses, val_corr_losses, val_att_losses, val_prof_losses, val_count_losses, val_input_seqs_array, val_profiles_array = run_epoch(val_loader, 'eval', val_model, 0, 1, cuda=1, shuffle=False, att_prior_loss_weight=230)\n",
    "print(\"VALIDATION\")\n",
    "print(\"overall\")\n",
    "print(np.nanmean(val_batch_losses))\n",
    "print(\"correctness\")\n",
    "print(np.nanmean(val_corr_losses))\n",
    "print(\"attribution\")\n",
    "print(np.nanmean(val_att_losses))\n",
    "print(\"profile\")\n",
    "print(np.nanmean(val_prof_losses))\n",
    "print(\"count\")\n",
    "print(np.nanmean(val_count_losses))\n",
    "\n",
    "print(\"------------------\")\n",
    "\n",
    "test_loader = proj.DataLoader(test_regions_npy_path,batch_size=1)\n",
    "test_model = proj.restore_model(\"./trained_models/exp6_prior_epoch_20.pt\", 1)\n",
    "test_model.cuda(1)\n",
    "test_model.eval()\n",
    "test_batch_losses, test_corr_losses, test_att_losses, test_prof_losses, test_count_losses, test_input_seqs_array, test_profiles_array = run_epoch(test_loader, 'eval', test_model, 0, 1, cuda=1, shuffle=False, att_prior_loss_weight=230)\n",
    "print(\"TESTING\")\n",
    "print(\"overall\")\n",
    "print(np.nanmean(test_batch_losses))\n",
    "print(\"correctness\")\n",
    "print(np.nanmean(test_corr_losses))\n",
    "print(\"attribution\")\n",
    "print(np.nanmean(test_att_losses))\n",
    "print(\"profile\")\n",
    "print(np.nanmean(test_prof_losses))\n",
    "print(\"count\")\n",
    "print(np.nanmean(test_count_losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
